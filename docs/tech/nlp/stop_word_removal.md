# 停用词去除

## 1. 概述

停用词去除是自然语言处理中的一个文本预处理步骤，其核心思想是将文本中那些出现频率很高、但对文本的核心含义贡献极小的词语过滤掉。

### 1.1 主要目的 
停用词去除的主要目的是**降维**和**聚焦**
    
 - **提升处理效率，节省资源**
   
    文本数据中可能有30%-50%甚至更多的词是停用词。移除它们可以大幅减少需要处理的词汇量，从而降低计算和存储成本，加快模型训练和查询速度。
 
 - **减少噪音，聚焦关键信息**
   
    在文本挖掘、情感分析、主题建模等任务中，我们更关心的是那些承载核心语义的实词（名词、动词、形容词等）。停用词会成为“噪音”，干扰模型对文本真正主题和情感倾向的判断。
 
 - **改善模型性能**

    通过去除无关信息，模型可以更专注于重要的词汇特征，从而通常能获得更好的效果和准确性。

### 1.2 停用词组成

停用词通常包括以下几类词语：

 - **功能词：** 用于构建语法结构，但本身没有太多实际含义。
   
    冠词：the, a, an（英语）
   
    介词：in, on, at, of, to, for, with
   
    连词：and, but, or, so, because
   
    代词：I, you, he, she, it, we, they
   
 - **常见动词：** is, am, are, was, were, have, has, do（作为助动词时）
 - **语气词和标点：** 嗯，啊，哦，的，地，得，了（中文）
 - **数字和单个字母**
  
**常见停用词：**

 - 中文：“的”, “了”, “在”, “是”, “我”, “有”, “和”, “就”, “不”, “人”, “都”, “也”......
 
 - 英文：“the”, “a”, “an”, “in”, “on”, “at”, “and”, “or”, “but”, “is”, “are”, “of”......

这里我们主要研究中文停用词的去除，英文暂时不做了解。

## 2. 去除停用词

### 2.1 主要去除方法

1. 使用现有的停用词表
2. 使用现成的工具库
4. 基于统计的方法自动生成停用词列表
5. 混合上述三种方法

### 2.2 使用现有的或自定义停用词表
使用jieba加载停用词表文件，进行停用词去除工作。常见的停用词表有哈工大停用词表、百度停用词表等，上述词表的txt文件见 [https://github.com/goto456/stopwords](https://github.com/goto456/stopwords)。

下面是使用哈工大停用词表的案例


```python
import jieba
"""加载停用词表"""
def load_stopwords(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        stopwords = set([line.strip() for line in f])
    return stopwords
    
"""去除停用词"""
def remove_stopwords(text, stopwords):
    # 分词
    words = jieba.cut(text)
    # 过滤停用词
    filtered_words = [word for word in words if word not in stopwords and word.strip() != '']
    return filtered_words

stopwords = load_stopwords('hit_stopwords.txt')
text = " 我今天早上去了一个很大的超市，然后我买了很多的东西，包括苹果、香蕉和牛奶。我觉得这个超市的东西真的很不错，而且价格也很便宜，所以我决定以后要经常来这里购物。"
result = remove_stopwords(text, stopwords)
print("原始文本:", text)
print("去除停用词后:", "/".join(result))
```
```
原始文本:  我今天早上去了一个很大的超市，然后我买了很多的东西，包括苹果、香蕉和牛奶。我觉得这个超市的东西真的很不错，而且价格也很便宜，所以我决定以后要经常来这里购物。
去除停用词后: 今天/早上/去/很大/超市/买/很多/东西/包括/苹果/香蕉/牛奶/觉得/超市/东西/真的/很/不错/价格/很/便宜/决定/以后/经常/购物
```   

### 2.3 使用现成的工具库

 #### 2.3.1 常见自然语言处理库

  - **NLTK：** 英文为主。功能全面，适合教学和研究，有助于深入理解算法原理。但速度较慢，且不易直接用于对性能和精度要求高的生产环境。
  - **spaCy：** 速度极快、精度高，提供预训练模型和API。是构建高性能英文NLP应用的首选，但在处理中文时精度较差。
  - **HanLP：** 中文首选。集成了最先进的深度学习模型，在中文分词、实体识别等任务上达到顶尖水平。
 
（这里暂时不展开，后续将进行补充）

### 2.4 基于统计的方法生成停用词表
基于统计方法自动生成停用词列表的核心原理是通过分析语料库中词语的分布特征，识别那些出现频率异常高、在不同文档中分布均匀且对文本区分贡献小的词语，主要依据词频、文档频率、TF-IDF值等统计指标，从中筛选出信息量低的高频词作为停用词。
